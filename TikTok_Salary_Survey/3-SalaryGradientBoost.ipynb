{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predicting Salary using Gradient Boosting Model**\n",
    "\n",
    "In this Notebook, we will focus on building a Gradient Boosting model to predict salary based on [`Age Range`, `Industry`, `Job Title`, `Education`, `Country`, `Gender`, `Experience`, `Annual Bonus` and `Signon Bonus`].\n",
    "\n",
    "the model is performing several tasks including data preprocessing, model training, hyperparameter tuning, model evaluation using cross-validation, and finally evaluating the model's performance on a hold-out test set. This process helps to build a predictive model for estimating annual salaries based on various features provided in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing the Model\n",
    "\n",
    "We will be taking the following steps to develop the model:\n",
    "1. Data Preparation\n",
    "2. Model Training: Training the model on the training dataset, adjusting parameters as needed to optimize performance.\n",
    "3. Model Evaluation: \n",
    "\n",
    "Hyperparameter Tuning: Use techniques like grid search or random search to find the optimal settings for your model's parameters. Iterate and Optimize: Based on the evaluation, refine the model by adjusting its parameters, reselecting features, or further addressing data imbalances. Repeat the training and evaluation process as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation/Preprocessing:\n",
    "\n",
    "Developing the Model:\n",
    "1.\tData Collection: Gather a dataset that is representative of the population you're studying. The quality and quantity of your data are crucial for building a reliable model.\n",
    "2.\tData Cleaning and Preprocessing: Clean the data to handle missing values, outliers, and errors. Preprocess the data by encoding categorical variables, normalizing numerical features, and splitting the dataset into training and testing sets.\n",
    "3.\tFeature Selection and Engineering: Identify the most relevant features that contribute to the target variable and create new features that could improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Cleaned_SalSur.csv')\n",
    "df_capped = pd.read_csv('Cleaned_SalSur.csv')\n",
    "\n",
    "# Outlier Detection using IQR\n",
    "Q1 = df[['Annual Salary', 'Annual Bonus', 'Signon Bonus']].quantile(0.25)\n",
    "Q3 = df[['Annual Salary', 'Annual Bonus', 'Signon Bonus']].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Count outliers\n",
    "outliers = ((df[['Annual Salary', 'Annual Bonus', 'Signon Bonus']] < lower_bound) | \n",
    "            (df[['Annual Salary', 'Annual Bonus', 'Signon Bonus']] > upper_bound)).sum()\n",
    "\n",
    "\n",
    "df_capped = df.copy()\n",
    "df_capped['Annual Salary'] = df_capped['Annual Salary'].clip(upper=upper_bound['Annual Salary'])\n",
    "df_capped['Annual Bonus'] = df_capped['Annual Bonus'].clip(upper=upper_bound['Annual Bonus'])\n",
    "\n",
    "\n",
    "# Define filters\n",
    "jobfilter = ['Audit Associate', 'Account Executive', 'Recruiter', 'Nurse', 'Sales',\n",
    "             'Actuarial Associate', 'Software Engineer', 'Project Manager',\n",
    "             'Account Manager', 'Teacher', 'Marketing', 'Coo', 'Consultant',\n",
    "             'Director', 'Engineer', 'Analyst', 'Manager']\n",
    "\n",
    "industryfilter = ['Computer Software', 'Construction', 'Architecture', 'Engineering', \n",
    "                  'Sales', 'Non-Profit', 'Law/Legal/Attorney', 'Advertising', \n",
    "                  'Banking', 'Insurance', 'Retail', 'Accounting', 'Consulting', \n",
    "                  'Marketing', 'Finance', 'Education', 'Information Technology/It', \n",
    "                  'Biotech', 'Health Care']\n",
    "\n",
    "countryfilter = ['United States', 'Canada', 'United Kingdom', 'Australia', 'Ireland']\n",
    "\n",
    "# Re-apply filters to the capped dataframe\n",
    "df_filtered_capped = df_capped[df_capped['Job Title'].isin(jobfilter) & \n",
    "                               df_capped['Industry'].isin(industryfilter) & \n",
    "                               df_capped['Country'].isin(countryfilter)]\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_capped.drop(['Annual Salary'], axis=1)\n",
    "y = df_capped['Annual Salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Parameter Tuning using Grid search:\n",
    "\n",
    "Define preprocessing steps for numeric and categorical features, including imputation and scaling for numeric features and one-hot encoding for categorical features.\n",
    "Combine preprocessing steps into a single ColumnTransformer.\n",
    "\n",
    "After conducting hyperparameter tuning using RandomizedSearchCV, the following hyperparameters were selected for the XGBoost model:\n",
    "\n",
    "- `n_estimators`: 300\n",
    "- `learning_rate`: 0.3\n",
    "- `max_depth`: 5\n",
    "- `subsample`: 0.7\n",
    "- `colsample_bytree`: 0.7\n",
    "- `reg_alpha`: 1\n",
    "- `reg_lambda`: 4\n",
    "- `gamma`: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps for numeric and categorical columns\n",
    "numeric_features = ['Experience', 'Annual Bonus', 'Signon Bonus']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['Age Range', 'Industry', 'Job Title', 'Education', 'Country', 'Gender']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Define the XGBRegressor with optimized hyperparameters\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=350, #300\n",
    "    learning_rate=0.3,\n",
    "    max_depth=5,\n",
    "    subsample=0.7,#1\n",
    "    colsample_bytree=0.7,#0.8\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=4,\n",
    "    gamma=3,\n",
    "    random_state=42)\n",
    "\n",
    "# Create a pipeline that includes the preprocessor and the model\n",
    "xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', xgb_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation:\n",
    "\n",
    "Validating the Model:\n",
    "1.\tCross-Validation: Use cross-validation techniques to assess how the model will generalize to an independent dataset. This involves dividing the dataset into complementary subsets, training the model on one subset, and validating it on the other.\n",
    "2.\tPerformance Metrics: Evaluate your model using metrics appropriate for classification tasks, such as accuracy, precision, recall, F1-score, and the area under the ROC curve (AUC). The choice of metrics should reflect your project's objectives and the balance between different types of errors.\n",
    "3.\tModel Interpretation: Understand how your model makes predictions and which features are most influential. This can help identify any biases or weaknesses in the model.\n",
    "4.\tEthical Consideration and Bias Mitigation: Assess and address potential biases in your model or data. Ensure that your model's use complies with ethical standards, especially when predicting sensitive information like gender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV MSE Scores: [4.45972173e+08 4.81263818e+08 4.81200522e+08 4.65374553e+08\n",
      " 4.67688888e+08]\n",
      "Mean CV MSE: 468299990.8142292\n",
      "Standard Deviation of CV MSE: 12976498.594821848\n",
      "Hold-out Test Set Mean Square Error(MSE): 446904196.1233598\n",
      "Hold-out Test Set R-Square: 0.6021761531567582\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation using KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(xgb_pipeline, X, y, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "cv_scores = np.abs(cv_scores)  # Convert scores to positive values\n",
    "\n",
    "# Print cross-validation MSE scores\n",
    "print(f\"CV MSE Scores: {cv_scores}\")\n",
    "print(f\"Mean CV MSE: {np.mean(cv_scores)}\")\n",
    "print(f\"Standard Deviation of CV MSE: {np.std(cv_scores)}\")\n",
    "\n",
    "# Split the capped data into training and testing sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate the performance metrics for the hold-out test set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "#print the results\n",
    "print(f\"Hold-out Test Set Mean Square Error(MSE): {mse}\")\n",
    "print(f\"Hold-out Test Set R-Square: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretation\n",
    "\n",
    "# Cross-Validation (CV) MSE Scores**\n",
    "\n",
    "**CV MSE Scores:** The MSE scores from cross-validation are relatively consistent, ranging from approximately 446 million to 481 million. This indicates that the model is somewhat stable across different subsets of the training data.\n",
    "\n",
    "**Mean CV MSE:**The average MSE across all CV folds is about 468 million. This value gives a general idea of the model's prediction error across the CV process.\n",
    "Standard Deviation of CV MSE: The standard deviation of the CV MSE scores is approximately 12.98 million, which is relatively small compared to the magnitude of the MSE values themselves. This low standard deviation suggests that the model's performance is quite consistent across different folds, indicating a stable model that does not suffer too much from variability due to the randomness in the data splitting.\n",
    "\n",
    "# Hold-out Test Set Performance\n",
    "**Hold-out Test Set MSE:** The MSE on the hold-out test set is approximately 446 million, which is slightly better (lower) than the mean CV MSE. This suggests that the model generalizes well to unseen data, at least as far as the MSE metric is concerned.\n",
    "\n",
    "**Hold-out Test Set R-Square (R²)**: The R² value of 0.602 indicates that about 60.22% of the variance in the salary is explained by the model. This is a decent level of predictive power, suggesting the model has learned meaningful patterns from the features that contribute to salary prediction.\n",
    "\n",
    "\n",
    "**Model Performance:** The gradient boosting model shows a good balance between bias and variance, as indicated by the consistency of CV MSE scores and the reasonable R² value on the hold-out test set. An R² of over 0.60 is indicative of a model that captures a significant portion of the variance in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Hyperparamter tuning using Randomize CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best parameters found:  {'model__colsample_bytree': 0.7418481581956125, 'model__gamma': 1.4607232426760908, 'model__learning_rate': 0.08327236865873834, 'model__max_depth': 8, 'model__n_estimators': 545, 'model__reg_alpha': 1.5703519227860272, 'model__reg_lambda': 1.5990213464750793, 'model__subsample': 0.8542703315240834}\n",
      "Lowest MSE found:  469876439.70147216\n",
      "Optimized Mean Square Error(MSE): 436818537.3437755\n",
      "Optimized R-Square: 0.6111541748635292\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Hyperparameter distribution\n",
    "param_dist = {\n",
    "    'model__n_estimators': randint(100, 600),\n",
    "    'model__learning_rate': uniform(0.01, 0.2),\n",
    "    'model__max_depth': randint(3, 10),\n",
    "    'model__subsample': uniform(0.7, 0.3),\n",
    "    'model__colsample_bytree': uniform(0.7, 0.3),\n",
    "    'model__reg_alpha': uniform(0, 2),\n",
    "    'model__reg_lambda': uniform(1, 3),\n",
    "    'model__gamma': uniform(0, 5)\n",
    "}\n",
    "\n",
    "# RandomizedSearch setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Number of parameter settings sampled\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearch\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Lowest MSE found: \", np.abs(random_search.best_score_))\n",
    "\n",
    "# Predict with best model\n",
    "y_pred_optimized = random_search.predict(X_test)\n",
    "\n",
    "# Performance Metrics\n",
    "mse_optimized = mean_squared_error(y_test, y_pred_optimized)\n",
    "r2_optimized = r2_score(y_test, y_pred_optimized)\n",
    "\n",
    "print(\"Optimized Mean Square Error(MSE):\", mse_optimized)\n",
    "print(\"Optimized R-Square:\", r2_optimized)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
