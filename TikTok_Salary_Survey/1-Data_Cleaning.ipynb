{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Providing Context:\n",
    "In 2021, a survey was conducted on TikTok to promote salary transparency and assist college students and employees in exploring career options and earning potential. Participants were asked to enter their salary and other information in a shared Google Sheets document, resulting in over 80,000 entries. Because this was an unstructured survey, answers lacked defined responses and standardized formatting. The survey contained irrelevant,incomplete responses and misspellings posing challenges for data cleaning and ensuring data integrity. Analyzing the data required extensive manual effort, including coding, categorizing, and interpreting responses.\n",
    "\n",
    "The responses to the survey were first processed in an SSIS package and loaded and then grouped into age ranges which was then loaded respectively into SQL tables and separate CSV files to speed up processing time.\n",
    "\n",
    "This notebook focuses on cleaning the data to prepare for visualization and machine learning models that will be performed in other notebooks. This is to help with readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Exploration, Analyis and Cleaning:**\n",
    "To tackle this project, I'll follow a structured approach, dividing the work into several steps to ensure clarity and effectiveness. Here's my roadmap:\n",
    "\n",
    "1.\tData Loading and Initial Exploration: Load the survey data to understand the structure, identify any apparent issues, and get a sense of the cleaning required.\n",
    "2.\tData Cleaning and Standardization: Clean the survey data by addressing missing values, correcting misspellings, standardizing categorical columns, and ensuring data types are appropriate and consistent for analysis.\n",
    "3.\tCategorization of all columns to standardize and preprocess them for machine learning modles. This will be broken down by column given the amount of processing each column requires.\n",
    "\n",
    "**For categorization:**\n",
    "1. Rename `Highest level of Education`, correct misspellings and group `Education` into categories.\n",
    "2. Reduce redundancy by group the `Industry` and `Job Title` columns. This will require some web scrapping and manual mapping.\n",
    "3. Correct and standardize misspellings in the `Country` column. \n",
    "4. Then Convert all salary columns into USD based on country's conversion rate to ensure accuracy and consistency. Currency conversion rates and country codes will be gotten through web scrapping as well.\n",
    "5. Finalize by grouping `Gender` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "files = ['SalarySur_18-21.csv', 'SalarySur_22-25.csv', 'SalarySur_26-29.csv',\n",
    "         'SalarySur_30-33.csv', 'SalarySur_34-37.csv', 'SalarySur_38-41.csv',\n",
    "         'SalarySur_42-49.csv', 'SalarySurv_58.csv', 'SalarySur_54-57.csv',\n",
    "         'SalarySur_50-53.csv']\n",
    "\n",
    "df = pd.concat([pd.read_csv(file) for file in files], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: data loading and initial exploration\n",
    "\n",
    "**Data Dictionary**\n",
    "\n",
    "The survey data is successfully loaded, revealing the structure and some initial insights. Here's a summary of the dataset's columns:\n",
    "\n",
    "•\t`Age Range`: Categorical, representing the participant's age group.\n",
    "\n",
    "•\t`Years of Experience`: Numerical, indicating how many years of professional experience the participant has.\n",
    "\n",
    "•\t`Industry`: Categorical, showing the industry in which the participant works.\n",
    "\n",
    "•\t`Job Title`: Categorical, listing the participant's job title.\n",
    "\n",
    "•\t`Highest Level of Education Received`: Categorical, detailing the highest level of education the participant has completed.\n",
    "\n",
    "•\t`Country`: Categorical, specifying the country of the participant's employment.\n",
    "\n",
    "•\t`Annual Salary`: Numerical, the participant's annual salary.\n",
    "\n",
    "•\t`Annual Bonus`: Numerical, the annual bonus amount the participant receives, if any.\n",
    "\n",
    "•\t`Signon Bonus`: Numerical, the sign-on bonus amount the participant received upon hiring, if any.\n",
    "\n",
    "•\t`Gender`: Categorical, indicating the participant's gender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  Age Range Years of Experience        Industry                      Job Title                Highest Level of Education Received      Country  Annual Salary  Annual Bonus  Signon Bonus  Gender\n",
       " 0     18-21                   1      Consulting                 Office Manager                                  Bachelor's Degree       CANADA        42000.0           0.0           0.0  Female\n",
       " 1     18-21                   1          Retail           Account Coordinator                                   Bachelor's Degree          USA        37000.0        1200.0           0.0  Female\n",
       " 2     18-21                   1  Transportation  Engineering product manager 1                                  Bachelor's Degree          USA        75000.0           0.0        3000.0  Female\n",
       " 3     18-21                   1      Accounting       Junior credit controller                       Some High School, No Diploma      ENGLAND        19500.0         250.0           0.0  Female\n",
       " 4     18-21                   1      Accounting   Junior Assistent accountant   High School Graduate, Diploma or the equivalen...  NETHERLANDS        30000.0        2200.0           0.0  Female,\n",
       " 48294)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displays first 5 rows\n",
    "df.head() , len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Values:**\n",
    "\n",
    "There are missing entries across multiple columns, with \"Industry\" and \"Job Title\" having the most significant number of missing values (726 and 792, respectively). Other columns with missing data include \"Years of Experience\", \"Annual Salary\", \"Annual Bonus\", \"Signon Bonus\", and a few others.\n",
    "\n",
    "**Summary Statistics:**\n",
    "\n",
    "These statistics provide a snapshot of the salary, annual bonus, and sign-on bonus distributions within the data set. the data suggests a varied distribution of salaries, annual bonuses, and sign-on bonuses, with a significant number of individuals not receiving bonuses or sign-on amounts. Additionally, there are outliers with exceptionally high salaries and bonuses. Further analysis could explore factors such as age range, years of experience, industry, job title, education level, country, and gender to understand their influence on salary and bonus distributions.\n",
    "\n",
    "From the statistics, we can infer that the majority of individuals in the dataset receive annual salaries ranging from $52,000 to $95,000, with a median salary of $70,000. Additionally, most reported annual bonuses and sign-on bonuses are zero or relatively low, with some extreme outliers indicating a small percentage of individuals receiving exceptionally high bonuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Range                                1\n",
      "Years of Experience                     10\n",
      "Industry                               726\n",
      "Job Title                              792\n",
      "Highest Level of Education Received      1\n",
      "Country                                  1\n",
      "Annual Salary                           61\n",
      "Annual Bonus                             3\n",
      "Signon Bonus                            19\n",
      "Gender                                   1\n",
      "dtype: int64        Annual Salary   Annual Bonus   Signon Bonus\n",
      "count   48233.000000   48291.000000   48275.000000\n",
      "mean    81582.805945    6447.878038    1482.807171\n",
      "std     56721.869219   21894.745465    9507.873636\n",
      "min         0.000000       0.000000       0.000000\n",
      "25%     52000.000000       0.000000       0.000000\n",
      "50%     70000.000000       0.000000       0.000000\n",
      "75%     95000.000000    5000.000000       0.000000\n",
      "max    990000.000000  750000.000000  715000.000000\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Summary statistics for numerical columns to identify any inconsistencies\n",
    "summary_statistics = df.describe()\n",
    "\n",
    "print(missing_values, summary_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detecting and handling outliers:**\n",
    "\n",
    "Overall, outlier detection using the IQR method serves as an important step in data preprocessing, contributing to improved data quality, robust statistical analyses, and better-performing machine learning models.\n",
    "\n",
    "The results indicate that there are 2461 outliers in the 'Annual Salary' column, 5979 outliers in the 'Annual Bonus' column, and 5875 outliers in the 'Signon Bonus' column. Upon further exploration, the outliers in the annual salaries are extermely high salary rates and the outliers in Annual Bonus and Signon Bonus are mostly zeros with some high bonuses. \n",
    "\n",
    "Employing IQR to handle missing values will result in loss of valuable information, especially since the missing values are not randomly distributed across the dataset. Using the IQR method to handle missing salary values doesn't take into account these dependencies. It treats each salary value in isolation, potentially leading to imprecise estimations that do not reflect the true distribution of salaries within different job titles or industries. Dropping rows based on the presence of missing values could lead to biased analyses or inaccurate modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers in Annual Salary:\n",
      "Annual Salary    2553\n",
      "Annual Bonus     6134\n",
      "Signon Bonus     6017\n",
      "dtype: int64\n",
      "      Age Range Years of Experience            Industry                      Job Title Highest Level of Education Received Country  Annual Salary  Annual Bonus  Signon Bonus      Gender\n",
      "2         18-21                   1      Transportation  Engineering product manager 1                   Bachelor's Degree     USA        75000.0           0.0        3000.0      Female\n",
      "9         18-21                   1          Accounting                    Assurance 1                   Bachelor's Degree  CANADA        55000.0           0.0        2000.0        Male\n",
      "10        18-21                   1          Accounting                Audit Associate                     Master's Degree     USA        63500.0           0.0        3000.0      Female\n",
      "11        18-21                   1          Accounting                       Sale Rep                   Bachelor's Degree     USA        50000.0           0.0        5000.0      Female\n",
      "12        18-21                   1          Accounting               Risk Consultant                    Bachelor's Degree     USA        62000.0           0.0        5000.0      Female\n",
      "...         ...                 ...                 ...                            ...                                 ...     ...            ...           ...           ...         ...\n",
      "48289     50-53                 20+        Electronics              Principal Engineer                    Doctorate Degree     USA       300000.0      200000.0           0.0        Male\n",
      "48290     50-53                 20+       Public sector                            CFO                   Bachelor's Degree  CANADA       600000.0      250000.0           0.0        Male\n",
      "48291     50-53                 20+       Manufacturing                            CEO                   Bachelor's Degree     USA       260000.0      260000.0           0.0        Male\n",
      "48292     50-53                 20+             Finance             Portfolio manager                  Professional Degree     USA       330000.0      670000.0           0.0        Male\n",
      "48293     50-53                  17  Payments (fintech)                           SVP                    Bachelor's Degree     USA       250000.0           0.0           NaN  Prefer Not\n",
      "\n",
      "[11787 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Outlier Detection using IQR\n",
    "Q1 = df[['Annual Salary', 'Annual Bonus', 'Signon Bonus']].quantile(0.25)\n",
    "Q3 = df[['Annual Salary', 'Annual Bonus', 'Signon Bonus']].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Calculating IQR Define bounds for outliers.\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Count outliers\n",
    "outlierscount = ((df[['Annual Salary', 'Annual Bonus', 'Signon Bonus']] < lower_bound) | \n",
    "            (df[['Annual Salary', 'Annual Bonus', 'Signon Bonus']] > upper_bound)).sum()\n",
    "\n",
    "# Identify outliers\n",
    "outliers = ((df[['Annual Salary', 'Annual Bonus', 'Signon Bonus']] < lower_bound) | \n",
    "            (df[['Annual Salary', 'Annual Bonus', 'Signon Bonus']] > upper_bound))\n",
    "\n",
    "# Print outliers\n",
    "print(\"Outliers in Annual Salary:\")\n",
    "print(outlierscount)\n",
    "\n",
    "# Filter DataFrame to include only rows with outliers\n",
    "outlier_rows = df[outliers.any(axis=1)]\n",
    "print(outlier_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first address Missing Values and decide how they'll be handled: \n",
    "\n",
    "Given the substantial number of the data set, and large number of null values in the industry and job titles column, the best approach is to drop the null values for simplification. Data set went from 48292 rows to 46889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46889\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 46889 entries, 0 to 48292\n",
      "Data columns (total 10 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   Age Range                            46889 non-null  object \n",
      " 1   Years of Experience                  46889 non-null  object \n",
      " 2   Industry                             46889 non-null  object \n",
      " 3   Job Title                            46889 non-null  object \n",
      " 4   Highest Level of Education Received  46889 non-null  object \n",
      " 5   Country                              46889 non-null  object \n",
      " 6   Annual Salary                        46889 non-null  float64\n",
      " 7   Annual Bonus                         46889 non-null  float64\n",
      " 8   Signon Bonus                         46889 non-null  float64\n",
      " 9   Gender                               46889 non-null  object \n",
      "dtypes: float64(3), object(7)\n",
      "memory usage: 3.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Age Range                              0\n",
       " Years of Experience                    0\n",
       " Industry                               0\n",
       " Job Title                              0\n",
       " Highest Level of Education Received    0\n",
       " Country                                0\n",
       " Annual Salary                          0\n",
       " Annual Bonus                           0\n",
       " Signon Bonus                           0\n",
       " Gender                                 0\n",
       " dtype: int64,\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping rows with any missing values\n",
    "df = df.dropna()\n",
    "\n",
    "#verify nulls have been removed\n",
    "df.isnull().sum(), print(len(df)), df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Cleaning, Standardizing and Categorization:\n",
    "**Standardizing columns for uniformity**\n",
    "\n",
    "**Categorical columns to be standardized:** Highest level of Education, Industry, Job title, Country, Salary columns, Gender in this order.\n",
    "\n",
    "As mentioned at the beginning, categorization will be done in steps:\n",
    "1. Rename 'Highest level of Education', correct misspellings and group Education into categories.\n",
    "2. categorizing 'Industry' This will require some web scrapping and manual mapping.\n",
    "3. Repeat for 'Job Title' columns. \n",
    "3. Correct and standardize misspellings in the 'Country' column. \n",
    "4. Then Convert all salary columns into USD based on country's conversion rate to ensure accuracy and consistency. Currency conversion rates and country codes will be gotten through web scrapping as well.\n",
    "5. Finalize by grouping 'Gender' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Categorizing 'Highest level of Education' column\n",
    "\n",
    "Renaming the column, correcting mispellings and categorizing values into the specified categories using a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Bachelor's Degree\", 'No Schooling', 'High School Diploma',\n",
       "       \"Master's Degree\", 'Associate Degree', 'Some college',\n",
       "       'Trade School', 'Doctoral Degree'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rename \"Highest Level of Education Received\" to \"Education\"\n",
    "df.rename(columns={'Highest Level of Education Received': 'Education'}, inplace=True)\n",
    "\n",
    "# Define a function to categorize the education levels\n",
    "def categorize_education(level):\n",
    "    #level = level.strip()\n",
    "    #detailed categories and keywords to match them\n",
    "    education_mappings = {\n",
    "        \"No Schooling\": ['Some High School, No Diploma','SomeHighSchool,NoDiploma','No Schooling Completed','NoSchoolingCompleted',\"N/a\"] ,\n",
    "        \"Trade School\": ['Trade School', 'Trade,Technical,VocationalTraining','Trade, Technical, Vocational Training'] ,\n",
    "        \"Some college\": [\"SomeCollegecredit,nodegree\",\"Some College credit, no degree\"],\n",
    "        \"High School Diploma\": [\"HighSchoolGraduate,Diplomaortheequivalent(e.g.GED)\",\"High School Graduate, Diploma or the equivalent (e\"] ,\n",
    "        \"Associate Degree\":['AssociateDegree', 'Associate Degree'] ,\n",
    "        \"Bachelor's Degree\": ['BachelorsDegree',\"Bachelor's Degree\"] ,\n",
    "        \"Master's Degree\": ['MastersDegree',\"Master's Degree\" ],\n",
    "        \"Doctoral Degree\": ['Doctorate Degree', 'DoctorateDegree',\"Professional Degree\",\"ProfessionalDegree\"] ,\n",
    "        \n",
    "    }\n",
    "    for category, keywords in education_mappings.items():\n",
    "        if any(keyword in level for keyword in keywords):\n",
    "            return category\n",
    "    return \"Other\" # For any education levels not explicitly mapped\n",
    "\n",
    "# Apply the categorization function to the \"Education\" column\n",
    "df['Education'] = df['Education'].apply(categorize_education)\n",
    "\n",
    "df['Education'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Categorizing Industry Column:\n",
    "\n",
    "steps:\n",
    "\n",
    "1. Webscrape to get a list of all industries from bls.gov and add to list that was mapped manually.\n",
    "2. Remove duplicate values by stripping trailing spaces and converting all entries to lower case\n",
    "3. using fuzzy matching to map the entries with an 80% match and other for non matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Getting List of Industries from bls.gov to use for Fuzzy matching:\n",
    "\n",
    "This script sends a request https://www.bls.gov/iag/tgs/iag_index_alpha.htm to extract the list of industry names and save them to the file industry_titles.txt which will be used to further standardize the column, by grouping similar entries together. The function iterates through all sections containing industry titles and extracts them. Each extracted industry title undergoes the same splitting process to exclude any additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizing and standardizing \"Country\" Column\n",
    "\n",
    "Correct the misspellings in the \"Country\" column and group similar countries together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to establish connection...\n",
      "Connection succeeded. Parsing HTML content...\n",
      "Scraping complete.\n",
      "Number of Industries: 115\n",
      "\n",
      "Industry Titles:\n",
      "Accommodation\n",
      "Accommodation and Food Services\n",
      "Administrative and Support Services\n",
      "Administrative and Support and Waste Management and Remediation Services\n",
      "Agriculture, Forestry, Fishing and Hunting\n",
      "\n",
      "Industry titles saved to 'industry_titles.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def scrape_industry_titles(url):\n",
    "    print(\"Attempting to establish connection...\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Referer': 'https://www.bls.gov/iag/tgs/iag_index_alpha.htm'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=60)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Failed to retrieve industry titles:\", str(e))\n",
    "        return None\n",
    "\n",
    "    print(\"Connection succeeded. Parsing HTML content...\")\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    industry_section = soup.find('div', class_='iag--tgs')\n",
    "\n",
    "    if not industry_section:\n",
    "        print(\"Industry section not found.\")\n",
    "        return None\n",
    "\n",
    "    # Extract industry titles\n",
    "    industry_titles = [title_element.text.strip().split(\" (N\")[0].strip() for title_element in industry_section.find_all('li')]\n",
    "    print(\"Scraping complete.\")\n",
    "    return industry_titles\n",
    "\n",
    "def save_to_csv(industry_titles, filename):\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Industry Titles\"])  # header\n",
    "        writer.writerows([[title.lower().strip()] for title in industry_titles])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.bls.gov/iag/tgs/iag_index_alpha.htm'\n",
    "    industry_titles = scrape_industry_titles(url)\n",
    "\n",
    "    if industry_titles:\n",
    "        print(\"Number of Industries:\", len(industry_titles))\n",
    "        print(\"\\nIndustry Titles:\")\n",
    "        for title in industry_titles[:5]:\n",
    "            print(title)\n",
    "\n",
    "        filename = 'industry_titles.csv'\n",
    "        save_to_csv(industry_titles, filename)\n",
    "        print(f\"\\nIndustry titles saved to '{filename}'.\")\n",
    "\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove duplicate values by stripping trailing spaces and converting all entries to lower case\n",
    "\n",
    "Industries: Standardizing formatting reduced entries from 8,424 down to 6,570.\n",
    "\n",
    "Efficiency: With the Industry column already cleaned and deduplicated, the fuzzy matching operation becomes significantly more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total industries:  8424\n",
      "Unique industries:  6487\n",
      "First 5 unique industries:  ['consulting' 'retail' 'transportation' 'accounting' 'advertising']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_industries = len(df['Industry'].unique())\n",
    "\n",
    "# Standardize formatting for \"Industry\" column\n",
    "df['Industry'] = df['Industry'].str.lower().str.strip()\n",
    "# After standardization, let's re-check the unique values for a sample comparison\n",
    "unique_industries = df['Industry'].unique()\n",
    "\n",
    "\n",
    "print(\"Total industries: \", all_industries)\n",
    "print(\"Unique industries: \", len(unique_industries))\n",
    "print(\"First 5 unique industries: \", unique_industries[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply fuzzy wuzzy to group entries:\n",
    "115 industries were extracted and over 88 were individually mapped. Further cleaning was done in Excel increasing the number of job titles from 115 to 296 to provide variety in mapping.\n",
    "Categorization Strategy:\n",
    "\n",
    "1. Fuzzy Matching: Given the potential for misspellings and variations, employing a technique like fuzzy matching can help match the survey entries to the cleaned lists of industries.\n",
    "2. Manual Review and Correction: Depending on the effectiveness of fuzzy matching, some manual review may be required for entries with low matching scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      Consulting\n",
      "1                          Retail\n",
      "2              Air Transportation\n",
      "3                      Accounting\n",
      "4                      Accounting\n",
      "                   ...           \n",
      "48288                     Biotech\n",
      "48289    Computer And Electronics\n",
      "48290                    Plumbing\n",
      "48291               Manufacturing\n",
      "48292                     Finance\n",
      "Name: Industry, Length: 46889, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from thefuzz import process\n",
    "\n",
    "# Load the industry_titles.csv\n",
    "with open('industry_titlesALL.csv', 'r') as file:\n",
    "    categories = file.read().splitlines()[1:]  # Remove header\n",
    "\n",
    "# Define a function for fuzzy matching\n",
    "def fuzzy_match_industries(unique_industries, categories):\n",
    "    matched_categories = {\n",
    "        industry: process.extractOne(industry, categories)[0]  # Get the best match\n",
    "        for industry in unique_industries\n",
    "        if not pd.isna(industry)  # Skip NaN values\n",
    "    }\n",
    "    \n",
    "    return matched_categories\n",
    "\n",
    "# Extract unique industries from the DataFrame\n",
    "unique_industries = df['Industry'].unique()\n",
    "\n",
    "# Apply fuzzy matching to unique industries\n",
    "matched_categories = fuzzy_match_industries(unique_industries, categories)\n",
    "\n",
    "# Map the entire DataFrame using this dictionary\n",
    "df['Industry'] = df['Industry'].map(matched_categories).fillna(\"Other\").str.title()\n",
    "\n",
    "print(df['Industry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Categorizing Job Title Column:\n",
    "\n",
    "follow the same steps performed for the industry column:\n",
    "\n",
    "1. Webscrape to get a list of all jobs from bls.gov and add to list that was mapped manually.\n",
    "2. remove duplicate values by stripping trailing spaces and converting all entries to lower case\n",
    "3. using fuzzy matching to map the entries with an 80% match and other for non matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Getting List of Job Titles from bls.gov to use for Fuzzy matching:**\n",
    "\n",
    "This script sends a request https://www.bls.gov/iag/tgs/iag_index_alpha.htm to extract the list of industry names and save them to the file industry_titles.csv which will be used to further standardize the column, by grouping similar entries together. The function iterates through all sections containing job titles and extracts job titles from each of them. Each extracted job title undergoes the same splitting process to exclude any additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to establish connection...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection succeeded. Parsing HTML content...\n",
      "Scraping complete.\n",
      "Number of Job Titles: 4763\n",
      "\n",
      "Job Titles:\n",
      "21 Dealer\n",
      "3D Animator\n",
      "3rd Grade Reading Teacher\n",
      "4th Grade Math Teacher\n",
      "911 Dispatcher\n",
      "\n",
      "Job titles saved to 'job_titles.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def scrape_job_titles(url):\n",
    "    print(\"Attempting to establish connection...\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Referer': 'https://www.bls.gov/ooh/a-z-index.htm#'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Failed to retrieve job titles:\", str(e))\n",
    "        return None\n",
    "\n",
    "    print(\"Connection succeeded. Parsing HTML content...\")\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    job_sections = soup.find_all('div', class_='a-z-list-box')\n",
    "\n",
    "    job_titles = []\n",
    "    for job_section in job_sections:\n",
    "        job_titles.extend([title_element.text.strip().split(\", see:\")[0].strip() for title_element in job_section.find_all('li')])\n",
    "\n",
    "    print(\"Scraping complete.\")\n",
    "    return job_titles\n",
    "\n",
    "def save_to_csv(job_titles, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows([title.lower().strip()] for title in job_titles)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.bls.gov/ooh/a-z-index.htm#'\n",
    "    job_titles = scrape_job_titles(url)\n",
    "\n",
    "    if job_titles:\n",
    "        print(\"Number of Job Titles:\", len(job_titles))\n",
    "        print(\"\\nJob Titles:\")\n",
    "        for title in job_titles[:5]:\n",
    "            print(title)\n",
    "\n",
    "        filename = 'job_titles.csv'\n",
    "        save_to_csv(job_titles, filename)\n",
    "        print(f\"\\nJob titles saved to '{filename}'.\")\n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Remove duplicate values by stripping trailing spaces and converting all entries to lower case**\n",
    "\n",
    "**Job Titles:** standardizing formatting reduced entries from 20,583 down to 15,717.\n",
    "\n",
    "**Efficiency:** With the \"Job Title\" already cleaned and deduplicated, the fuzzy matching operation becomes significantly more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total job titles: 20361\n",
      "Number of unique job titles after standardization: 15717\n",
      "Unique job titles: ['office manager' 'account coordinator' 'engineering product manager 1'\n",
      " ... 'diagnostic imaging engineer'\n",
      " 'certified regulatory compliance manager' 'regional account manager']\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of unique job titles\n",
    "all_jobs = len(df['Job Title'].unique())\n",
    "\n",
    "# Standardize formatting for \"Industry\" and \"Job Title\" columns\n",
    "df['Job Title'] = df['Job Title'].str.lower().str.strip()\n",
    "\n",
    "# Check the unique job titles after standardization\n",
    "unique_job_titles = df['Job Title'].unique()\n",
    "\n",
    "# Print the total number of unique job titles, before and after standardization\n",
    "print(\"Total job titles:\", all_jobs)\n",
    "print(\"Number of unique job titles after standardization:\", len(unique_job_titles))\n",
    "print(\"Unique job titles:\", unique_job_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Apply fuzzy wuzzy to group entries:**\n",
    "\n",
    "Over 4500 job titles were extracted from bls which were then combined with a lists of job titles that were manually mapped(88). Further cleaning was done in Excel reducing the number of job titles significnatly from 5000 to 2500 to help facilitate and simplify the fuzzy match process. Still, with over 2500 job titles to match agaisnt, a different approach needs to taken:\n",
    "\n",
    "1. **Parallel Processing:** Use parallel processing libraries like joblib to distribute the fuzzy matching workload across multiple CPU cores. By setting n_jobs= -1 in Parallel, we utilize all available CPU cores, speeding up the matching process.\n",
    "2. **Batch Processing:** Break down the list of unique job titles into smaller batches and perform fuzzy matching on these batches. This can help manage memory usage and improve responsiveness.\n",
    "3. **Manual Review and Correction:** Depending on the effectiveness of fuzzy matching, some manual review may be required for entries with low matching scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Business Office Manager\n",
      "1            Account Coordinator\n",
      "2                       Engineer\n",
      "3                     Controller\n",
      "4                     Accountant\n",
      "                  ...           \n",
      "48288                         Vp\n",
      "48289                   Engineer\n",
      "48290                      Other\n",
      "48291                        Ceo\n",
      "48292                    Manager\n",
      "Name: Job Title, Length: 46889, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from thefuzz import process\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Standardize job titles in the DataFrame\n",
    "df['Job Title'] = df['Job Title'].str.lower().str.strip()\n",
    "\n",
    "# Load and clean job titles from CSV\n",
    "with open('job_titlesALL.csv', 'r', encoding='latin-1') as file: \n",
    "    job_titles = file.read().splitlines()[1:] # Remove header\n",
    "\n",
    "#Function to fuzzy match job titles\n",
    "def fuzzy_match_job(job, job_list):\n",
    "    best_match, score = process.extractOne(job, job_list)\n",
    "    return best_match if score > 73 else \"Other\"\n",
    "\n",
    "# Fuzzy match unique job titles in parallel\n",
    "matched_jobs = Parallel(n_jobs=-1)(delayed(fuzzy_match_job)(title, job_titles) for title in df['Job Title'].unique())\n",
    "\n",
    "# Create a dictionary of matched job titles\n",
    "matched_jobs_dict = dict(zip(df['Job Title'].unique(), matched_jobs))\n",
    "\n",
    "# Map the job titles in the DataFrame using the matched dictionary\n",
    "df['Job Title'] = df['Job Title'].map(matched_jobs_dict).fillna(\"Other\").str.title()\n",
    "\n",
    "print(df['Job Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying \"Industry\" and \"Job Title\" column mappings\n",
    "\n",
    "The `Industry` column went from 6,487 unique entries to 173 unique entries after the standardization.\n",
    "\n",
    "The `Job Title` column went from 15,717 unique entries to 1,115 unique entries after the standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of unique Industries: 173\n",
      "Current number of unique Job Titles: 1115\n"
     ]
    }
   ],
   "source": [
    "print(\"Current number of unique Industries:\", len(df['Industry'].unique()))\n",
    "\n",
    "print(\"Current number of unique Job Titles:\" ,len(df['Job Title'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Standardizing 'Country' Column: \n",
    "1. Inspect column\n",
    "2. clean and standardize. Group if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CANADA', 'USA', 'ENGLAND', 'NETHERLANDS', 'AUSTRALIA',\n",
       "       'NEW ZEALAND', 'IRELAND', 'TURKEY ', 'SRILANKA', 'SINGAPORE',\n",
       "       'PHILIPPINES', 'ENGLAND ', 'SOUTH KOREA', 'ICELAND', 'SCOTLAND ',\n",
       "       'MEXICO', 'MONACO', 'LITHUANIA', 'NEW ZEALAND ', 'NORWAY', 'QATAR',\n",
       "       'IRELAND ', 'GERMANY', 'SPAIN', 'SINGAPORE ', 'ARGENTINA',\n",
       "       'FINLAND', 'JAPAN', 'FRANCE', 'AFGHANISTAN', 'SWEDEN', 'BELIZE',\n",
       "       'SCOTLAND', 'RUSSIA', 'SAUDI ARABIA ', 'BANGLADESH', 'UAE',\n",
       "       'SWITZERLAND', 'POLAND', 'GERMANY ', 'CHILE ', 'HUNGARY ',\n",
       "       'PORTUGAL', 'MOROCCO', 'BELGIUM', 'THAILAND ', 'CAMBODIA ',\n",
       "       'DENMARK', 'NETHERLANDS ', 'MALAYSIA', 'LITHUANIA ',\n",
       "       'CAYMAN ISLANDS', 'CHILE', 'COLOMBIA', 'PERU', 'PUERTO RICO',\n",
       "       'ISRAEL', 'ALBANIA', 'BRAZIL', 'SWITZERLAND ', 'SERBIA', 'LATVIA',\n",
       "       'GUATEMALA', 'SOUTH AFRICA', 'EGYPT', 'SPAIN ', 'ITALY ',\n",
       "       'POLAND ', 'BULGARIA', 'DOMINICAN REPUBLIC ', 'BULGARIA ',\n",
       "       'ROMANIA', 'PANAMÁ', 'AFGHANISTAN ', 'BAHRAIN', 'HONDRUAS ',\n",
       "       'FRANCE ', 'SAUDI ARABIA', 'MALDIVES', 'JAMAICA', 'TAIWAN',\n",
       "       'CHINA', 'ESTONIA', 'ECUADOR', 'CAYMAN ISLANDS ', 'GERMANY  ',\n",
       "       'KENYA', 'BRITISH COLUMBIA ', 'ENGLANDRAINE', 'HUNGARY', 'PANAMA',\n",
       "       'TRINIDAD & TOBAGO', 'PARAGUAY ', 'PAKISTAN', 'INDIA',\n",
       "       'SOUTH AFRICA ', 'EL SALVADOR ', 'ITALY', 'SWITZERAND',\n",
       "       'HONG KONG', 'SUDAN', 'LEBANON', 'BELGIUM '], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will first inspect the unique values to identify the corrections needed\n",
    "df['Country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Standardizing column:**\n",
    "\n",
    "Having inspected the unique values in the `Country` column, they are a few issues to correct, such as trailing spaces and variations in country names that should be grouped together. For example, \"USA\" and variations like \"united states of america\" should all be standardized to \"USA\". The list also includes \"ENGLAND\" and \"SCOTLAND\", which can be grouped under \"UK\" for simplicity, along with any other variations like \"britain\".\n",
    "\n",
    "Also, correcting mispellings and formatting at this stage will to get conversion rates per country to accurately calculate all salaries in USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Canada', 'United States', 'United Kingdom', 'Netherlands',\n",
       "       'Australia', 'New Zealand', 'Ireland', 'Turkey', 'Sri Lanka',\n",
       "       'Singapore', 'Philippines', 'South Korea', 'Iceland', 'Mexico',\n",
       "       'Monaco', 'Lithuania', 'Norway', 'Qatar', 'Germany', 'Spain',\n",
       "       'Argentina', 'Finland', 'Japan', 'France', 'Afghanistan', 'Sweden',\n",
       "       'Belize', 'Russia', 'Saudi Arabia', 'Bangladesh',\n",
       "       'United Arab Emirates', 'Switzerland', 'Poland', 'Chile',\n",
       "       'Hungary', 'Portugal', 'Morocco', 'Belgium', 'Thailand',\n",
       "       'Cambodia', 'Denmark', 'Malaysia', 'Cayman Islands', 'Colombia',\n",
       "       'Peru', 'Puerto Rico', 'Israel', 'Albania', 'Brazil', 'Serbia',\n",
       "       'Latvia', 'Guatemala', 'South Africa', 'Egypt', 'Italy',\n",
       "       'Bulgaria', 'Dominican Republic', 'Romania', 'Panama', 'Bahrain',\n",
       "       'Honduras', 'Maldives', 'Jamaica', 'Taiwan', 'China', 'Estonia',\n",
       "       'Ecuador', 'Kenya', 'Trinidad And Tobago', 'Paraguay', 'Pakistan',\n",
       "       'India', 'El Salvador', 'Hong Kong', 'Sudan', 'Lebanon'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct and group country names\n",
    "\n",
    "def standardize_country_names(country):\n",
    "    country = country.strip().upper()  # Normalize case and strip whitespace\n",
    "    # Map to group countries\n",
    "    country_mapping = {\n",
    "        'UNITED STATES': 'USA',\n",
    "        'UNITED KINGDOM': ['ENGLAND', 'BRITAIN', 'ENGLANDRAINE', 'SCOTLAND'],\n",
    "        'SWITZERLAND': 'SWITZERAND',\n",
    "        'UNITED ARAB EMIRATES': 'UAE',\n",
    "        'CANADA': 'BRITISH COLUMBIA',\n",
    "        \"SRI LANKA\": \"SRILANKA\",\n",
    "        \"HONDURAS\": \"HONDRUAS\",\n",
    "        \"TRINIDAD AND TOBAGO\": \"TRINIDAD & TOBAGO\",\n",
    "        \"PANAMA\": ['PANAMA', 'PANAMÁ']\n",
    "        # Add more mappings if necessary\n",
    "    }\n",
    "\n",
    "    for key, value in country_mapping.items():\n",
    "        if country in value:\n",
    "            return key\n",
    "    \n",
    "    return country\n",
    "\n",
    "# Apply the function to the \"Country\" column\n",
    "df['Country'] = df['Country'].apply(standardize_country_names).str.title()\n",
    "df['Country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Converting \"Annual Salary\", \"Annual Bonus\" and \"Annual Signon Bonus\" to USD based on participant country.\n",
    "\n",
    "It is to be assumed that particpants entered their salaries based upon the country in which they reside.\n",
    "\n",
    "**Categorizing All Salary Columns. A 3 step approach:**\n",
    "1. Verify that country names in the `Country` column match how they're listed on \n",
    "2. Fetch for currendy codes for the unique list of countries in our data frame\n",
    "3. Fetch exchange rates using country codes we got in step 2.\n",
    "4. Use the exchange rates and create a function that converts `Annual Salary`, `Annual Bonus` and `Signon Bonus` columns to USD.\n",
    "\n",
    "Performing all the steps above allows for consistent and accurate salary predictions for all countries based on USD. If not, an entry for South Africa within the data set has a an annual salary for 380,000 which is approximately (SAR 10,889,627.76) which is grossly inaccurate based on research showing the median to highest salary for an engineer in South African is between (ZAR 450K - 1.4M). If the salary is not converted to USD, that would make S.A. amongst the highest salaries which skews the results. After the salary conversions, the ne salary becomes 20814.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age Range Years of Experience                     Industry            Job Title        Education       Country  Annual Salary  Annual Bonus  Signon Bonus  Gender\n",
      "13618     22-25                   3                   Accounting              Auditor  Doctoral Degree  South Africa      390000.00       40000.0           0.0    Male\n",
      "14672     22-25                   3  Rental And Leasing Services              Analyst  Master's Degree  South Africa      461309.99           0.0           0.0  Female\n",
      "19075     22-25                   5              Branding/Design     Graphic Designer     No Schooling  South Africa       54000.00           0.0           0.0    Male\n",
      "27678     26-29                   5                  Health Care  Account Coordinator     Some college  South Africa       53000.00           0.0           0.0  Female\n",
      "39123     30-33                   8                      Biotech                Other     Some college  South Africa       40000.00           0.0           0.0  Female\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a specific country to view the annual salary before conversion\n",
    "desired_countries = ['South Africa'] #'China','South Korea','Ireland', \n",
    "\n",
    "# Filter the DataFrame for rows where the 'Country' column matches any of the desired countries\n",
    "filtered_df = df[df['Country'].isin(desired_countries)]\n",
    "print(filtered_df[:5] )\n",
    "df['Annual Salary'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Verify the countries in the \"Country\" column are named properly.** \n",
    "\n",
    "The function will return a dictionary with country names as keys and their currency codes as values. For countries with multiple currencies, only the first currency code listed is returned. This dictionary is is then stored to a file and used to standardize the country names in your DataFrame to match the API's format.\n",
    "\n",
    "With this dictionary, we can cross-reference the country names in the `Country` column and correct any discrepancies. This will help ensure that all subsequent API calls for currency codes based on country names are successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Cyprus', 'EUR'), ('Eritrea', 'ERN'), ('Liberia', 'LRD'), ('Bermuda', 'BMD'), ('Vatican City', 'EUR'), ('Cook Islands', 'CKD'), ('Somalia', 'SOS'), ('Zambia', 'ZMW'), ('Venezuela', 'VES'), ('Turkmenistan', 'TMT')]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "def get_all_countries_currencies():\n",
    "    \"\"\"\n",
    "    Fetch all country names and their corresponding currency codes from the REST Countries API.\n",
    "    :return: Dictionary mapping country names to currency codes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # REST Countries API endpoint for all countries\n",
    "        response = requests.get(\"https://restcountries.com/v3.1/all\")\n",
    "        data = response.json()\n",
    "        # Dictionary comprehension to extract countries and their currencies\n",
    "        countries_currencies = {country['name']['common']: list(country['currencies'].keys())[0]\n",
    "                                for country in data if 'currencies' in country}\n",
    "        return countries_currencies\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching countries and currencies: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Fetch all countries and currencies\n",
    "all_countries_currencies = get_all_countries_currencies()\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('country_currency_codes.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Country', 'Currency Code'])  # Write header\n",
    "    for country, currency in all_countries_currencies.items():\n",
    "        writer.writerow([country, currency])\n",
    "\n",
    "\n",
    "# Preview the first 5 entries\n",
    "preview = list(all_countries_currencies.items())[:10]\n",
    "print(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Fetch for currendy codes for the unique list of countries in the data frame.**\n",
    "\n",
    "\n",
    "This code defines a function `get_country_currency_mapping(countries)` that takes a list of country names as input. It then iterates over each country in the input list, makes a request to the REST Countries API to fetch data about the country, extracts the currency code from the response, and stores it in a dictionary mapping each country name to its corresponding currency code. If an error occurs during the process, it prints an error message.\n",
    "\n",
    "I manually set the currency code for Ireland because it kept getting GDP which is incorrect.\n",
    "\n",
    "This will be used to extract conversion rates to calculate the salaries in the following section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Canada', 'CAD'), ('United States', 'USD'), ('United Kingdom', 'GBP'), ('Netherlands', 'EUR'), ('Australia', 'AUD'), ('New Zealand', 'NZD'), ('Ireland', 'EUR'), ('Turkey', 'TRY'), ('Sri Lanka', 'LKR'), ('Singapore', 'SGD')]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_country_currency_mapping(countries):\n",
    "    \"\"\"\n",
    "    Fetch currency codes for a given list of countries using the REST Countries API.\n",
    "    :param countries: List of country names.\n",
    "    :return: Dictionary mapping country names to currency codes.\n",
    "    \"\"\"\n",
    "    country_currency = {}\n",
    "    for country in countries:\n",
    "        try:\n",
    "            if country == \"Ireland\":  # Check for Ireland and assign the correct currency code\n",
    "                country_currency[country] = \"EUR\"\n",
    "            else:\n",
    "                # REST Countries API endpoint; adjust as necessary\n",
    "                response = requests.get(f\"https://restcountries.com/v3.1/name/{country}\")\n",
    "                data = response.json()\n",
    "                # Extracting the first currency code for the country\n",
    "                currency_code = list(data[0]['currencies'].keys())[0]\n",
    "                country_currency[country] = currency_code\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching currency for {country}: {e}\")\n",
    "    return country_currency\n",
    "\n",
    "# Example usage with a list of unique countries from your dataset\n",
    "unique_countries = df['Country'].unique().tolist()\n",
    "country_currency_mapping = get_country_currency_mapping(unique_countries)\n",
    "\n",
    "# Preview the first 5 entries\n",
    "preview = list(country_currency_mapping.items())[:10]\n",
    "print(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Fetch exchange rates using country codes we got in step 2.**\n",
    "\n",
    "The `fetch_exchange_rate` function uses `get_country_currency_mapping(unique_countries)` to extract conversion rate for each country. The `convert_salary `function then multiplies the `Annual Salary`, `Annual Bonus` and `Signon Bonus` by the rate to convert the currency to USD. The function is then applied to the columns.\n",
    "\n",
    "This code defines a function `fetch_exchange_rate(api_key, base_currency)`responsible for retrieving the exchange rate for a given base currency against USD from the ExchangeRate-API. If the base currency is USD, it returns 1.0 since the exchange rate for USD to USD is 1.0. The function first checks if the base currency is USD. If it is, it returns 1.0. Otherwise, it constructs the API endpoint URL using the provided API key and base currency. It then sends a request to the API, retrieves the response in JSON format, and extracts the conversion rate if the request is successful. If the request fails, it prints an error message and returns None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exchange rates:  {'CAD': 0.7391, 'USD': 1.0, 'GBP': 1.275, 'EUR': 1.0888, 'AUD': 0.6556, 'NZD': 0.6063, 'TRY': 0.03084, 'LKR': 0.00329, 'SGD': 0.7449, 'PHP': 0.0178, 'KRW': 0.00074829, 'ISK': 0.007299, 'MXN': 0.05971, 'NOK': 0.09414, 'QAR': 0.2747, 'ARS': 0.001172, 'JPY': 0.006614, 'AFN': 0.01402, 'SEK': 0.09585, 'BZD': 0.5, 'RUB': 0.01082, 'SAR': 0.2667, 'BDT': 0.009114, 'AED': 0.2723, 'CHF': 1.1262, 'PLN': 0.2516, 'CLP': 0.001035, 'HUF': 0.00276, 'MAD': 0.09952, 'THB': 0.02772, 'KHR': 0.00024728, 'DKK': 0.1462, 'MYR': 0.2111, 'KYD': 1.2, 'COP': 0.00025794, 'PEN': 0.2705, 'ILS': 0.2735, 'ALL': 0.01053, 'BRL': 0.199, 'RSD': 0.009267, 'GTQ': 0.1283, 'ZAR': 0.05329, 'EGP': 0.0213, 'BGN': 0.5563, 'DOP': 0.01692, 'RON': 0.2181, 'PAB': 1.0, 'BHD': 2.6596, 'HNL': 0.04053, 'MVR': 0.06486, 'JMD': 0.006499, 'TWD': 0.03143, 'KES': 0.00755, 'TTD': 0.1477, 'PYG': 0.00013681, 'PKR': 0.003589, 'HKD': 0.1278, 'SDG': 0.001877, 'LBP': 1.117e-05}\n",
      "          Country  Annual Salary  Annual Bonus  Signon Bonus\n",
      "0          Canada        31042.2          0.00           0.0\n",
      "1   United States        37000.0       1200.00           0.0\n",
      "2   United States        75000.0          0.00        3000.0\n",
      "3  United Kingdom        24862.5        318.75           0.0\n",
      "4     Netherlands        32664.0       2395.36           0.0\n"
     ]
    }
   ],
   "source": [
    "def fetch_exchange_rate(api_key, base_currency):\n",
    "    \"\"\"\n",
    "    Fetches the exchange rate to USD for the given base currency using the ExchangeRate-API.\n",
    "    Assumes 1 USD exchange rate for USD to simplify conversions.\n",
    "    \"\"\"\n",
    "    if base_currency == \"USD\":\n",
    "        return 1.0\n",
    "    url = f\"https://v6.exchangerate-api.com/v6/{api_key}/pair/{base_currency}/USD\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if data['result'] == 'success':\n",
    "        return data['conversion_rate']\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for {base_currency}\")\n",
    "        return None\n",
    "\n",
    "api_key = \"f59d7a6da77e535013faad78\"\n",
    "# Ensure this uses the correct mapping between country names and their currency codes\n",
    "exchange_rates = {currency: fetch_exchange_rate(api_key, currency) for currency in country_currency_mapping.values()}\n",
    "print(\"Exchange rates: \",exchange_rates)\n",
    "\n",
    "# Corrected function to convert salary columns to USD\n",
    "def convert_salary(row, exchange_rates, country_currency_mapping):\n",
    "    \"\"\"this function ensures that salary data in different currencies are converted to USD, except for \n",
    "    salaries already denoted in USD. Then rounds each converted salary to 2 dec places.\"\"\"\n",
    "    currency_code = country_currency_mapping.get(row['Country'], 'USD')\n",
    "    rate = exchange_rates.get(currency_code, 1.0)  # Default to 1.0 for USD or missing rates\n",
    "    \n",
    "    for col in ['Annual Salary', 'Annual Bonus', 'Signon Bonus']:\n",
    "        if pd.notnull(row[col]):\n",
    "            #converts the salaries to USD for all countries except the US\n",
    "            row[col] = round(row[col] * rate, 2) if currency_code != 'USD' else round(row[col], 2)\n",
    "    return row\n",
    "\n",
    "# Apply the conversion with corrected logic\n",
    "df = df.apply(lambda row: convert_salary(row, exchange_rates, country_currency_mapping), axis=1)\n",
    "\n",
    "# Check the results\n",
    "print(df[['Country', 'Annual Salary', 'Annual Bonus', 'Signon Bonus']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Finalize by grouping gender column.\n",
    "1. Remove special characters (+) and Rename `Years of Experience` to `Experience`.\n",
    "2. Standardize the `Gender` Column: edit descriptions to a consistent format, accounting for common variations.\n",
    "3. Save all cleaned files to *Cleaned_Sal_Sur.csv* to be used for part 2 and 3 of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Female', 'Male', 'Non-Binary', 'Other'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1: Replace \"20+\" with \"20\" in the 'Years of Experience' column\n",
    "df['Years of Experience'] = df['Years of Experience'].str.replace('20\\+', '20', regex=True)\n",
    "\n",
    "#Rename \"Years of Experience\" to \"Experience\"\n",
    "df.rename(columns={'Years of Experience': 'Experience'}, inplace=True)\n",
    "\n",
    "# Step 2: Standardize the Gender Column\n",
    "def standardize_gender(gender):\n",
    "    gender = gender.strip().lower()\n",
    "    if 'female' in gender:\n",
    "        return 'Female'\n",
    "    elif 'male' in gender:\n",
    "        return 'Male'\n",
    "    elif 'non-binary' in gender or 'nonbinary' in gender:\n",
    "        return 'Non-Binary'\n",
    "    else:\n",
    "        return 'Other'\n",
    "#apply function to gender column\n",
    "df['Gender'] = df['Gender'].apply(standardize_gender)\n",
    "\n",
    "#Save cleaned file for ML models and visualizations.\n",
    "df.to_csv('Cleaned_SalSur.csv', index=False)\n",
    "\n",
    "df['Gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age Range</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Education</th>\n",
       "      <th>Country</th>\n",
       "      <th>Annual Salary</th>\n",
       "      <th>Annual Bonus</th>\n",
       "      <th>Signon Bonus</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Consulting</td>\n",
       "      <td>Business Office Manager</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Canada</td>\n",
       "      <td>31042.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Account Coordinator</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>United States</td>\n",
       "      <td>37000.0</td>\n",
       "      <td>1200.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Air Transportation</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>United States</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>Controller</td>\n",
       "      <td>No Schooling</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>24862.5</td>\n",
       "      <td>318.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>32664.0</td>\n",
       "      <td>2395.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18-21</td>\n",
       "      <td>2</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>Actuarial Associate</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>31237.5</td>\n",
       "      <td>1912.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>Staff Accountant</td>\n",
       "      <td>Master's Degree</td>\n",
       "      <td>United States</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>Intern</td>\n",
       "      <td>Associate Degree</td>\n",
       "      <td>United States</td>\n",
       "      <td>44160.0</td>\n",
       "      <td>500.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>Consultant</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>United States</td>\n",
       "      <td>66600.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>Quality Assurance Analysts And Testers</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Canada</td>\n",
       "      <td>40650.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1478.2</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Age Range Experience            Industry                               Job Title            Education         Country  Annual Salary  Annual Bonus  Signon Bonus  Gender\n",
       "0     18-21          1          Consulting                 Business Office Manager    Bachelor's Degree          Canada        31042.2          0.00           0.0  Female\n",
       "1     18-21          1              Retail                     Account Coordinator    Bachelor's Degree   United States        37000.0       1200.00           0.0  Female\n",
       "2     18-21          1  Air Transportation                                Engineer    Bachelor's Degree   United States        75000.0          0.00        3000.0  Female\n",
       "3     18-21          1          Accounting                              Controller         No Schooling  United Kingdom        24862.5        318.75           0.0  Female\n",
       "4     18-21          1          Accounting                              Accountant  High School Diploma     Netherlands        32664.0       2395.36           0.0  Female\n",
       "5     18-21          2          Accounting                     Actuarial Associate  High School Diploma  United Kingdom        31237.5       1912.50           0.0  Female\n",
       "6     18-21          1          Accounting                        Staff Accountant      Master's Degree   United States        58000.0          0.00           0.0  Female\n",
       "7     18-21          1          Accounting                                  Intern     Associate Degree   United States        44160.0        500.00           0.0  Female\n",
       "8     18-21          1          Accounting                              Consultant    Bachelor's Degree   United States        66600.0          0.00           0.0    Male\n",
       "9     18-21          1          Accounting  Quality Assurance Analysts And Testers    Bachelor's Degree          Canada        40650.5          0.00        1478.2    Male"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview of cleaned and formatted columns\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
